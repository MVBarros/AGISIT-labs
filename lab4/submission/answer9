[miguel@miguel-laptop webfront]$  ab -n 10000 -c 2 http://127.0.0.1:8080/
This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking 127.0.0.1 (be patient)
Completed 1000 requests
Completed 2000 requests
Completed 3000 requests
Completed 4000 requests
Completed 5000 requests
Completed 6000 requests
Completed 7000 requests
Completed 8000 requests
Completed 9000 requests
Completed 10000 requests
Finished 10000 requests


Server Software:        nginx
Server Hostname:        127.0.0.1
Server Port:            8080

Document Path:          /
Document Length:        933 bytes

Concurrency Level:      2
Time taken for tests:   8.637 seconds
Complete requests:      10000
Failed requests:        0
Total transferred:      11390000 bytes
HTML transferred:       9330000 bytes
Requests per second:    1157.76 [#/sec] (mean)
Time per request:       1.727 [ms] (mean)
Time per request:       0.864 [ms] (mean, across all concurrent requests)
Transfer rate:          1287.78 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    0   0.0      0       2
Processing:     1    2   0.3      2       6
Waiting:        1    2   0.3      2       6
Total:          1    2   0.3      2       6

Percentage of the requests served within a certain time (ms)
  50%      2
  66%      2
  75%      2
  80%      2
  90%      2
  95%      2
  98%      2
  99%      3
 100%      6 (longest request)


Increasing the number of requests does not cause the system to drop requests nor does it slow the time it takes per request. Only the total time of the benchmark as an effective increase, as expected. Since only two requests are sent at once, the system can easily handle the load, even if it is prolonged, has each server is never attending more than one request.

Analysing the stats page after increasing the number of requests of the benchmark shows a higher number of total sessions and bytes in the frontend and backend, but session rate and max sessions stays the same (as is expected, no server is processing more than one request at a time)


[miguel@miguel-laptop webfront]$  ab -n 1000 -c 20 http://127.0.0.1:8080/
This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking 127.0.0.1 (be patient)
Completed 100 requests
Completed 200 requests
Completed 300 requests
Completed 400 requests
Completed 500 requests
Completed 600 requests
Completed 700 requests
Completed 800 requests
Completed 900 requests
Completed 1000 requests
Finished 1000 requests


Server Software:        nginx
Server Hostname:        127.0.0.1
Server Port:            8080

Document Path:          /
Document Length:        933 bytes

Concurrency Level:      20
Time taken for tests:   0.629 seconds
Complete requests:      1000
Failed requests:        0
Total transferred:      1139000 bytes
HTML transferred:       933000 bytes
Requests per second:    1590.70 [#/sec] (mean)
Time per request:       12.573 [ms] (mean)
Time per request:       0.629 [ms] (mean, across all concurrent requests)
Transfer rate:          1769.35 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    0   0.1      0       1
Processing:     3   12   3.0     11      24
Waiting:        2   12   3.0     11      24
Total:          3   12   3.0     12      24

Percentage of the requests served within a certain time (ms)
  50%     12
  66%     13
  75%     14
  80%     14
  90%     16
  95%     18
  98%     20
  99%     21
 100%     24 (longest request)


Increasing the concurrency parameter results in a higher time per request. This is because there are only 6 webservers, but 20 requests are sent at once, so some requests end up competing for resources in the same server. Looking at the stats page, no request was queued or dropped.

Increasing the level of concurrency increases the maximum number of sessions, i.e. the maximum number of requests each server processed at the same time.

The stats page also indicates that all requests are evenly distributed. This is because all web servers are active and have the same weight, so the load balancing is effectively a round-robin.


In both cases there were no failed requests, the system was able to handle the load
